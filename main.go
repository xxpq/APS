package main

import (
	"container/list"
	"context"
	"crypto/tls"
	"database/sql"
	"flag"
	"fmt"
	"log"
	"net"
	"net/http"
	"net/url"
	"os"
	"os/signal"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"syscall"
	"time"

	"golang.org/x/net/http2"
	"golang.org/x/net/http2/h2c"
	_ "modernc.org/sqlite"
)

// ServerManager manages the lifecycle of multiple HTTP servers.
type ServerManager struct {
	servers    map[string]*http.Server
	tcpServers map[string]*RawTCPServer  // Raw TCP servers
	udpServers map[string]*RawUDPServer  // Raw UDP servers
	muxes      map[string]*ConnectionMux // Connection multiplexers
	mu         sync.Mutex
	wg         sync.WaitGroup
	config     *Config
	configFile string
	// dataStore     *DataStore // Removed, replaced by statsDB for persistence
	harManager     *HarLoggerManager
	tunnelManager  TunnelManagerInterface
	scriptRunner   *ScriptRunner
	trafficShaper  *TrafficShaper
	stats          *StatsCollector
	staticCache    *StaticCacheManager
	replayManager  *ReplayManager
	statsDB        *StatsDB
	loggingDB      *LoggingDB
	logBroadcaster *LogBroadcaster
	rateLimiter    *RateLimitEngine
}

func NewServerManager(config *Config, configFile string, harManager *HarLoggerManager, tunnelManager TunnelManagerInterface, scriptRunner *ScriptRunner, trafficShaper *TrafficShaper, stats *StatsCollector, staticCache *StaticCacheManager, replayManager *ReplayManager, statsDB *StatsDB, loggingDB *LoggingDB, logBroadcaster *LogBroadcaster) *ServerManager {
	rateLimiter := NewRateLimitEngine(config.RateLimitRules)
	// go rateLimiter.CleanupExpired() // RateLimitEngine handles cleanup internally or doesn't need explicit cleanup loop yet?
	// The new engine uses sync.Map and doesn't have a cleanup loop yet.
	// We should probably add one, but for now let's just initialize it.
	// The old one had CleanupExpired. The new one has trackers that might grow.
	// WindowTracker resets itself. Banned map might grow.
	// We can add a cleanup goroutine later if needed.

	return &ServerManager{
		servers:        make(map[string]*http.Server),
		tcpServers:     make(map[string]*RawTCPServer),
		udpServers:     make(map[string]*RawUDPServer),
		muxes:          make(map[string]*ConnectionMux),
		config:         config,
		configFile:     configFile,
		harManager:     harManager,
		tunnelManager:  tunnelManager,
		scriptRunner:   scriptRunner,
		trafficShaper:  trafficShaper,
		stats:          stats,
		staticCache:    staticCache,
		replayManager:  replayManager,
		statsDB:        statsDB,
		loggingDB:      loggingDB,
		logBroadcaster: logBroadcaster,
		rateLimiter:    rateLimiter,
	}
}

func (sm *ServerManager) Start(name string, serverConfig *ListenConfig, isACMEEnabled bool) {
	sm.mu.Lock()
	defer sm.mu.Unlock()

	// Check if already running (HTTP or TCP)
	if _, exists := sm.servers[name]; exists {
		log.Printf("Server '%s' is already running.", name)
		return
	}
	if _, exists := sm.tcpServers[name]; exists {
		log.Printf("TCP Server '%s' is already running.", name)
		return
	}
	if _, exists := sm.udpServers[name]; exists {
		log.Printf("UDP Server '%s' is already running.", name)
		return
	}

	// Re-calculate mappings for this specific server
	serverMappings := make(map[string][]*Mapping)
	for i := range sm.config.Mappings {
		mapping := &sm.config.Mappings[i]
		// First, add mappings that explicitly specify this server
		for _, serverName := range mapping.serverNames {
			serverMappings[serverName] = append(serverMappings[serverName], mapping)
		}

		// For rawTCP servers, also match by port if no explicit server assignment
		if (serverConfig.Type == ServerTypeTCP || serverConfig.Type == ServerTypeTCPUDP) && len(mapping.serverNames) == 0 {
			fromURL := mapping.GetFromURL()
			if strings.HasPrefix(fromURL, "tcp://") {
				// Parse the from URL to get the port
				if u, err := url.Parse(fromURL); err == nil {
					if portStr := u.Port(); portStr != "" {
						if mappingPort, err := strconv.Atoi(portStr); err == nil {
							if mappingPort == serverConfig.Port {
								serverMappings[name] = append(serverMappings[name], mapping)
								log.Printf("[RAW TCP] Auto-assigned mapping %s to server '%s' (port %d)", fromURL, name, serverConfig.Port)
							}
						}
					}
				}
			}
		}

		// For rawUDP servers, also match by port if no explicit server assignment
		if (serverConfig.Type == ServerTypeUDP || serverConfig.Type == ServerTypeTCPUDP || serverConfig.Type == ServerTypeHTTPUDP) && len(mapping.serverNames) == 0 {
			fromURL := mapping.GetFromURL()
			if strings.HasPrefix(fromURL, "udp://") {
				// Parse the from URL to get the port
				if u, err := url.Parse(fromURL); err == nil {
					if portStr := u.Port(); portStr != "" {
						if mappingPort, err := strconv.Atoi(portStr); err == nil {
							if mappingPort == serverConfig.Port {
								serverMappings[name] = append(serverMappings[name], mapping)
								log.Printf("[RAW UDP] Auto-assigned mapping %s to server '%s' (port %d)", fromURL, name, serverConfig.Port)
							}
						}
					}
				}
			}
		}
	}

	// Start TCP Server if enabled (Type 1 or 4)
	if serverConfig.Type == ServerTypeTCP || serverConfig.Type == ServerTypeTCPUDP {
		tcpServer := NewRawTCPServer(name, serverConfig, sm.config, serverMappings[name],
			sm.tunnelManager, sm.trafficShaper, sm.stats, sm.loggingDB)
		if err := tcpServer.Start(); err != nil {
			log.Printf("Failed to start TCP server '%s': %v", name, err)
			// If TCP fails, we might still want to try UDP if it's combined?
			// For now, let's just log and continue, or return?
			// If it's pure TCP, we should probably return.
			if serverConfig.Type == ServerTypeTCP {
				return
			}
		} else {
			sm.tcpServers[name] = tcpServer
			log.Printf("[RAW TCP] Server '%s' started on port %d with %d mappings", name, serverConfig.Port, len(serverMappings[name]))
		}
	}

	// Start UDP Server if enabled (Type 3, 4, or 5)
	if serverConfig.Type == ServerTypeUDP || serverConfig.Type == ServerTypeTCPUDP || serverConfig.Type == ServerTypeHTTPUDP {
		udpServer := NewRawUDPServer(name, serverConfig, sm.config, serverMappings[name],
			sm.tunnelManager, sm.trafficShaper, sm.stats, sm.loggingDB)
		if err := udpServer.Start(); err != nil {
			log.Printf("Failed to start UDP server '%s': %v", name, err)
			if serverConfig.Type == ServerTypeUDP {
				return
			}
		} else {
			sm.udpServers[name] = udpServer
			log.Printf("[RAW UDP] Server '%s' started on port %d with %d mappings", name, serverConfig.Port, len(serverMappings[name]))
		}
	}

	// Start HTTP Server if enabled (Type 2 or 5)
	// Note: Type 0 defaults to HTTP in config processing, but here we check explicitly
	if serverConfig.Type == ServerTypeHTTP || serverConfig.Type == ServerTypeHTTPUDP {
		handler := createServerHandler(name, serverMappings[name], serverConfig, sm.config, sm.configFile, sm.harManager, sm.tunnelManager, sm.scriptRunner, sm.trafficShaper, sm.stats, sm.staticCache, sm.replayManager, isACMEEnabled, sm.statsDB, sm.loggingDB, sm.logBroadcaster, sm.rateLimiter)
		server, mux := startServer(name, serverConfig, handler, sm.tunnelManager, sm.rateLimiter)
		if server != nil {
			sm.servers[name] = server
			if mux != nil {
				sm.muxes[name] = mux
			}
			sm.wg.Add(1)
			go func() {
				defer sm.wg.Done()
				// The server's ListenAndServe/Serve method will block here.
				// When it returns (e.g., after Shutdown), the goroutine will exit.
			}()
		}
	}
}

func (sm *ServerManager) Stop(name string) {
	sm.mu.Lock()
	defer sm.mu.Unlock()

	// Try to stop HTTP server
	if server, exists := sm.servers[name]; exists {
		log.Printf("Stopping server '%s'...", name)
		// Use a context to allow for a graceful shutdown.
		ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
		defer cancel()
		if err := server.Shutdown(ctx); err != nil {
			log.Printf("Error shutting down server '%s': %v", name, err)
		}
		delete(sm.servers, name)

		if mux, exists := sm.muxes[name]; exists {
			mux.Stop()
			delete(sm.muxes, name)
		}

		log.Printf("HTTP Server '%s' stopped.", name)
	}

	// Try to stop TCP server
	if tcpServer, exists := sm.tcpServers[name]; exists {
		log.Printf("Stopping TCP server '%s'...", name)
		if err := tcpServer.Stop(); err != nil {
			log.Printf("Error stopping TCP server '%s': %v", name, err)
		}
		delete(sm.tcpServers, name)
		log.Printf("TCP Server '%s' stopped.", name)
	}

	// Try to stop UDP server
	if udpServer, exists := sm.udpServers[name]; exists {
		log.Printf("Stopping UDP server '%s'...", name)
		if err := udpServer.Stop(); err != nil {
			log.Printf("Error stopping UDP server '%s': %v", name, err)
		}
		delete(sm.udpServers, name)
		log.Printf("UDP Server '%s' stopped.", name)
	}
}

// UpdateRawTCPMappings updates mappings for all rawTCP servers (for config hot reload)
func (sm *ServerManager) UpdateRawTCPMappings() {
	sm.mu.Lock()
	defer sm.mu.Unlock()

	// Re-calculate mappings for all servers using the same logic as server startup
	serverMappings := make(map[string][]*Mapping)

	// First, collect mappings that explicitly specify servers
	for i := range sm.config.Mappings {
		mapping := &sm.config.Mappings[i]
		for _, serverName := range mapping.serverNames {
			serverMappings[serverName] = append(serverMappings[serverName], mapping)
		}
	}

	// Then, for each rawTCP server, also match TCP mappings by port
	for name, tcpServer := range sm.tcpServers {
		serverPort := tcpServer.config.Port

		// Look for TCP mappings without explicit server assignment that match this port
		for i := range sm.config.Mappings {
			mapping := &sm.config.Mappings[i]

			// Skip if already assigned via serverNames
			if len(mapping.serverNames) > 0 {
				continue
			}

			fromURL := mapping.GetFromURL()
			if !strings.HasPrefix(fromURL, "tcp://") {
				continue
			}

			// Parse the from URL to get the port
			u, err := url.Parse(fromURL)
			if err != nil {
				continue
			}

			portStr := u.Port()
			if portStr == "" {
				continue
			}

			mappingPort, err := strconv.Atoi(portStr)
			if err != nil {
				continue
			}

			// If ports match, add this mapping
			if mappingPort == serverPort {
				serverMappings[name] = append(serverMappings[name], mapping)
			}
		}
	}

	// Update each rawTCP server's mappings
	for name, tcpServer := range sm.tcpServers {
		tcpServer.UpdateMappings(serverMappings[name])
	}

	// Also update UDP mappings
	sm.UpdateUDPMappings()
}

// UpdateUDPMappings updates mappings for all rawUDP servers
func (sm *ServerManager) UpdateUDPMappings() {
	// Re-calculate mappings for all servers using the same logic as server startup
	serverMappings := make(map[string][]*Mapping)

	// First, collect mappings that explicitly specify servers
	for i := range sm.config.Mappings {
		mapping := &sm.config.Mappings[i]
		for _, serverName := range mapping.serverNames {
			serverMappings[serverName] = append(serverMappings[serverName], mapping)
		}
	}

	// Then, for each rawUDP server, also match UDP mappings by port
	for name, udpServer := range sm.udpServers {
		serverPort := udpServer.config.Port

		// Look for UDP mappings without explicit server assignment that match this port
		for i := range sm.config.Mappings {
			mapping := &sm.config.Mappings[i]

			// Skip if already assigned via serverNames
			if len(mapping.serverNames) > 0 {
				continue
			}

			fromURL := mapping.GetFromURL()
			if !strings.HasPrefix(fromURL, "udp://") {
				continue
			}

			// Parse the from URL to get the port
			u, err := url.Parse(fromURL)
			if err != nil {
				continue
			}

			portStr := u.Port()
			if portStr == "" {
				continue
			}

			mappingPort, err := strconv.Atoi(portStr)
			if err != nil {
				continue
			}

			// If ports match, add this mapping
			if mappingPort == serverPort {
				serverMappings[name] = append(serverMappings[name], mapping)
			}
		}
	}

	// Update each rawUDP server's mappings
	for name, udpServer := range sm.udpServers {
		udpServer.UpdateMappings(serverMappings[name])
	}
}

func (sm *ServerManager) StopAll() {
	sm.mu.Lock()
	names := make([]string, 0, len(sm.servers)+len(sm.tcpServers))
	for name := range sm.servers {
		names = append(names, name)
	}
	for name := range sm.tcpServers {
		names = append(names, name)
	}
	for name := range sm.udpServers {
		names = append(names, name)
	}
	// Muxes are stopped when their corresponding server is stopped, but we should ensure cleanup
	// No need to iterate muxes separately as they are keyed by server name
	sm.mu.Unlock()

	for _, name := range names {
		sm.Stop(name)
	}
	sm.wg.Wait()
}

func main() {
	configFile := flag.String("config", "config.json", "Path to configuration file")
	flag.Parse()

	log.Println("===========================================")
	log.Println("  Any Proxy Service (APS) v1.0.0")
	log.Println("===========================================")

	if err := InitCertificates(); err != nil {
		log.Fatalf("Failed to initialize certificates: %v", err)
	}

	config, err := LoadConfig(*configFile)
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}

	InitACME(config)

	// Initialize shared database
	db, err := sql.Open("sqlite", "aps.db")
	if err != nil {
		log.Fatalf("Failed to open database: %v", err)
	}
	defer db.Close()

	// Set connection pool settings for SQLite
	db.SetMaxOpenConns(1)
	db.SetMaxIdleConns(1)

	log.Printf("[DB] Opened shared database: aps.db")

	// Initialize statistics module with shared database
	statsDB, err := NewStatsDB(db)
	if err != nil {
		log.Fatalf("Failed to initialize stats DB: %v", err)
	}
	defer statsDB.Close()

	// Initialize logging module with shared database
	loggingDB, err := NewLoggingDB(db)
	if err != nil {
		log.Fatalf("Failed to initialize logging DB: %v", err)
	}
	defer loggingDB.Close()

	//Initialize ASN cache with shared database
	globalASNCache, err = NewASNCache(db, 1000)
	if err != nil {
		log.Printf("[ASN] Failed to initialize ASN cache: %v (continuing without database caching)", err)
		// Create minimal cache without database
		globalASNCache = &ASNCache{
			memoryCache: make(map[string]*cacheEntry),
			lruList:     list.New(),
			maxEntries:  1000,
			httpClient: &http.Client{
				Timeout: 10 * time.Second,
			},
			apiURL: "https://api.ipapi.is/",
		}
	}

	// Initialize LogBroadcaster to capture logs for SSE
	logBroadcaster := NewLogBroadcaster(os.Stderr)
	log.SetOutput(logBroadcaster)

	// Load initial quota usage from DB
	initialQuotaUsage, err := statsDB.LoadAllQuotaUsage()
	if err != nil {
		log.Fatalf("Failed to load initial quota usage from DB: %v", err)
	}

	harManager := NewHarLoggerManager(config)
	defer harManager.Shutdown()

	tunnelManager := NewHybridTunnelManager(config, nil) // ‰ΩøÁî®Ê∑∑ÂêàÈößÈÅìÁÆ°ÁêÜÂô®
	scriptRunner := NewScriptRunner(config.Scripting)
	trafficShaper := NewTrafficShaper(initialQuotaUsage)
	statsCollector := NewStatsCollector(config)
	defer statsCollector.Close() // Ensure graceful shutdown of async stats workers

	// ÂàùÂßãÂåñÈùôÊÄÅÊñá‰ª∂ÁºìÂ≠òÁÆ°ÁêÜÂô®
	staticCache := NewStaticCacheManager(config.StaticCache)
	defer staticCache.Stop()

	// ËÆæÁΩÆtunnelManagerÁöÑstatsCollectorÔºåÂÆûÁé∞Á´ØÁÇπÁªüËÆ°ÁöÑÈõÜ‰∏≠ÂºèÁÆ°ÁêÜ
	tunnelManager.SetStatsCollector(statsCollector)
	replayManager := NewReplayManager(config)

	serverManager := NewServerManager(config, *configFile, harManager, tunnelManager, scriptRunner, trafficShaper, statsCollector, staticCache, replayManager, statsDB, loggingDB, logBroadcaster)

	watcher, err := NewConfigWatcher(*configFile, config, serverManager)
	if err != nil {
		log.Fatalf("Failed to create config watcher: %v", err)
	}
	watcher.Start()
	defer watcher.Stop()

	serverManager.StartAll()

	// Start quota persistence (now saving to DB)
	startQuotaPersistence(trafficShaper, statsDB)
	startStatsCollection(statsCollector, statsDB)
	startLogCleanup(config, loggingDB)

	log.Println("===========================================")
	log.Printf("Loaded %d mapping rules:", len(config.Mappings))
	for i, mapping := range config.Mappings {
		log.Printf("  [%d] %s -> %s (on %v)", i+1, mapping.GetFromURL(), mapping.GetToURL(), mapping.serverNames)
	}
	log.Println("===========================================")
	fmt.Println()
	fmt.Println("üîê HTTPS Interception Setup:")
	fmt.Println("   1. Configure your system or browser to use one of the proxy servers.")
	fmt.Println("   2. Visit '/.ssl' on any server with 'cert: \"auto\"' to download the root certificate.")
	fmt.Println()

	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, os.Interrupt, syscall.SIGTERM)

	<-sigChan
	log.Println("\nShutting down servers...")
	serverManager.StopAll()
}

func (sm *ServerManager) StartAll() {
	sm.config.mu.RLock()
	defer sm.config.mu.RUnlock()

	// Ê£ÄÊü•ÊòØÂê¶ÊúâÊúçÂä°ÈÖçÁΩÆ‰∫ÜACME
	needsACMEChallengeServer := false
	for _, serverConfig := range sm.config.Servers {
		if certStr, ok := serverConfig.Cert.(string); ok && certStr == "acme" {
			needsACMEChallengeServer = true
			break
		}
	}

	// Â¶ÇÊûúÈúÄË¶ÅACMEÔºåÁ°Æ‰øùÊúâ‰∏Ä‰∏™ÂÖ¨ÂÖ±ÁöÑ80Á´ØÂè£ÊúçÂä°Âô®
	if needsACMEChallengeServer {
		foundPort80 := false
		for _, serverConfig := range sm.config.Servers {
			if serverConfig.Port == 80 && (serverConfig.Public == nil || *serverConfig.Public) {
				foundPort80 = true
				break
			}
		}
		if !foundPort80 {
			log.Println("[ACME] No public server on port 80 found, creating one for ACME challenge.")
			acmeServerName := "acme_challenge_server"
			t := true
			sm.config.Servers[acmeServerName] = &ListenConfig{
				Port:   80,
				Public: &t,
			}
		}
	}

	// Â∞Ü mappings Êåâ server name ÂàÜÁªÑ
	serverMappings := make(map[string][]*Mapping)
	for i := range sm.config.Mappings {
		mapping := &sm.config.Mappings[i]
		for _, serverName := range mapping.serverNames {
			serverMappings[serverName] = append(serverMappings[serverName], mapping)
		}
	}

	// ‰∏∫ÊØè‰∏™ server ÂàõÂª∫Âπ∂ÂêØÂä®‰∏Ä‰∏™Â§ÑÁêÜÂô®
	for name, serverConfig := range sm.config.Servers {
		if serverConfig == nil {
			continue
		}
		sm.Start(name, serverConfig, needsACMEChallengeServer)
	}
}

func createServerHandler(serverName string, mappings []*Mapping, serverConfig *ListenConfig, config *Config, configFile string, harManager *HarLoggerManager, tunnelManager TunnelManagerInterface, scriptRunner *ScriptRunner, trafficShaper *TrafficShaper, stats *StatsCollector, staticCache *StaticCacheManager, replayManager *ReplayManager, isACMEEnabled bool, statsDB *StatsDB, loggingDB *LoggingDB, logBroadcaster *LogBroadcaster, rateLimiter *RateLimitEngine) http.Handler {
	mux := http.NewServeMux()
	proxy := NewMapRemoteProxy(config, harManager, tunnelManager, scriptRunner, trafficShaper, stats, staticCache, loggingDB, serverName, rateLimiter)

	// Â¶ÇÊûú cert ÊòØ autoÔºåÊ≥®ÂÜåËØÅ‰π¶‰∏ãËΩΩÂ§ÑÁêÜÂô®
	if certStr, ok := serverConfig.Cert.(string); ok && certStr == "auto" {
		certHandlers := &CertHandlers{}
		certHandlers.RegisterHandlers(mux)
	}

	// Ê≥®ÂÜå Auth ÁÆ°ÁêÜÊé•Âè£
	authHandlers := &AuthHandlers{}
	authHandlers.RegisterHandlers(mux)

	// Ê∑ªÂä†ÈáçÊîæÁ´ØÁÇπÔºàÂßãÁªàÂèØÁî®Ôºâ
	mux.HandleFunc("/.replay", replayManager.ServeHTTP)

	// Ê†πÊçÆ panel ÊéßÂà∂ /.api ‰∏é /.admin ÁöÑÊ≥®ÂÜå
	if serverConfig.Panel != nil && *serverConfig.Panel {
		// Ê∑ªÂä†ÁªüËÆ°Êï∞ÊçÆÁ´ØÁÇπ
		mux.HandleFunc("/.api/stats", stats.ServeHTTP)

		// Ê≥®ÂÜåÁÆ°ÁêÜÈù¢ÊùøÂ§ÑÁêÜÂô®
		adminHandlers := NewAdminHandlers(config, configFile, stats, statsDB, loggingDB, logBroadcaster, rateLimiter)
		// ËÆæÁΩÆtunnelÁÆ°ÁêÜÂô®ÂºïÁî®ÔºåÁî®‰∫éÊü•ËØ¢endpointÁä∂ÊÄÅ
		adminHandlers.SetTunnelManager(tunnelManager)
		adminHandlers.RegisterHandlers(mux)
	}

	// ÂàõÂª∫‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂ§ÑÁêÜÂô®Êù•Â§ÑÁêÜÊâÄÊúâËØ∑Ê±Ç
	var baseHandler http.Handler = http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// ‰ª£ÁêÜËØ∑Ê±Ç (CONNECT)
		if r.Method == http.MethodConnect {
			proxy.ServeHTTP(w, r)
			return
		}

		// Ê£ÄÊü• mux ‰∏≠ÊòØÂê¶ÊúâÊõ¥ÂÖ∑‰ΩìÁöÑÂåπÈÖç (‰æãÂ¶ÇËØÅ‰π¶‰∏ãËΩΩÈ°µÈù¢ÊàñÁõ∏ÂØπË∑ØÂæÑ)
		handler, pattern := mux.Handler(r)
		if pattern != "" {
			handler.ServeHTTP(w, r)
			return
		}

		// ÈªòËÆ§Â§ÑÁêÜ HTTP ËØ∑Ê±ÇËΩ¨Âèë
		proxy.ServeHTTP(w, r)
	})

	// Â¶ÇÊûúÊòØ80Á´ØÂè£ÔºåÂπ∂‰∏îÂÖ®Â±ÄÂêØÁî®‰∫ÜACMEÔºåÂàôÂåÖË£ÖÂ§ÑÁêÜÂô®‰ª•Â§ÑÁêÜACMEÊåëÊàò
	if serverConfig.Port == 80 && isACMEEnabled {
		baseHandler = GetACMEHandler(baseHandler)
	}

	// Configure HTTP/2 server with strict limits to prevent goroutine leaks
	http2Server := &http2.Server{
		MaxConcurrentStreams:         100,              // Limit concurrent streams per connection
		MaxReadFrameSize:             64 << 10,         // 64kb max frame size
		IdleTimeout:                  10 * time.Second, // Close idle connections
		MaxUploadBufferPerConnection: 64 << 10,         // 64kb buffer per connection
		MaxUploadBufferPerStream:     64 << 10,         // 64kb buffer per stream
	}

	// Use h2c with configured server
	return h2c.NewHandler(baseHandler, http2Server)
}

func startServer(name string, config *ListenConfig, handler http.Handler, tunnelManager TunnelManagerInterface, rateLimiter *RateLimitEngine) (*http.Server, *ConnectionMux) {
	// Determine bind address based on 'public' (default: true)
	host := "127.0.0.1"
	if config.Public == nil || *config.Public {
		host = "0.0.0.0"
	}
	addr := fmt.Sprintf("%s:%d", host, config.Port)
	server := &http.Server{
		Handler:           handler,
		WriteTimeout:      30 * time.Second, // Kill stuck writes after 30s
		ReadHeaderTimeout: 10 * time.Second, // Already set elsewhere, consolidating here
		IdleTimeout:       60 * time.Second, // Close idle connections
	}

	log.Printf("Starting server '%s' on %s", name, addr)

	// Create listener manually
	listener, err := net.Listen("tcp", addr)
	if err != nil {
		log.Printf("Failed to listen on %s for server '%s': %v", addr, name, err)
		return nil, nil
	}

	// Create ConnectionMux
	mux := NewConnectionMux(listener)
	mux.SetRateLimiter(rateLimiter, name, config.RateLimitRules)

	// Setup Tunnel Handler
	mux.SetTunnelHandler(func(conn net.Conn) {
		tunnelManager.HandleTunnelConnection(conn)
	})

	// Setup HTTP Handler
	httpListener := NewChannelListener(listener.Addr())
	mux.SetHTTPHandler(func(conn net.Conn) {
		httpListener.Push(conn)
	})

	// Start Mux
	go mux.Start()

	if config.Cert != nil {
		// HTTPS server
		go func() {
			tlsConfig := &tls.Config{}
			var err error

			if cert, ok := config.Cert.(CertFiles); ok {
				tlsConfig.Certificates = make([]tls.Certificate, 1)
				tlsConfig.Certificates[0], err = tls.LoadX509KeyPair(cert.Cert, cert.Key)
				if err != nil {
					log.Printf("Failed to load certificate for server '%s': %v", name, err)
					return
				}
			} else if config.Cert == "auto" {
				tlsConfig.GetCertificate = func(info *tls.ClientHelloInfo) (*tls.Certificate, error) {
					return GenerateCertForHost(info.ServerName)
				}
			} else if config.Cert == "acme" {
				acmeTLSConfig := GetACMETLSConfig()
				if acmeTLSConfig == nil {
					log.Printf("ACME manager not initialized for server '%s', cannot start HTTPS server.", name)
					return
				}
				tlsConfig = acmeTLSConfig
			}

			tlsListener := NewTlsListener(httpListener, tlsConfig)
			if err := server.Serve(tlsListener); err != nil && err != http.ErrServerClosed {
				log.Printf("Server '%s' (HTTPS) failed: %v", name, err)
			}
		}()
	} else {
		// HTTP server
		go func() {
			if err := server.Serve(httpListener); err != nil && err != http.ErrServerClosed {
				log.Printf("Server '%s' (HTTP) failed: %v", name, err)
			}
		}()
	}
	return server, mux
}
func startQuotaPersistence(trafficShaper *TrafficShaper, statsDB *StatsDB) {
	ticker := time.NewTicker(10 * time.Second)
	go func() {
		for range ticker.C {
			trafficShaper.quotas.Range(func(key, value interface{}) bool {
				sourceKey := key.(string)
				var trafficUsed, requestsUsed int64
				if tq, ok := value.(*TrafficQuota); ok {
					trafficUsed = tq.Used
				}
				if rq, ok := value.(*RequestQuota); ok {
					requestsUsed = rq.Used
				}
				if err := statsDB.SaveQuotaUsage(sourceKey, trafficUsed, requestsUsed); err != nil {
					log.Printf("[QUOTA] Error saving quota usage to DB for %s: %v", sourceKey, err)
				}
				return true
			})
		}
	}()
}

func startStatsCollection(stats *StatsCollector, statsDB *StatsDB) {
	ticker := time.NewTicker(1 * time.Minute)
	go func() {
		for range ticker.C {
			snapshot := TimeSeriesSnapshot{
				Timestamp: time.Now().Unix(),
				Global: GlobalStats{
					TotalRequests:     atomic.LoadUint64(&stats.TotalRequests),
					ActiveConnections: atomic.LoadInt64(&stats.ActiveConnections),
					BytesReceived:     atomic.LoadUint64(&stats.TotalBytesRecv),
					BytesSent:         atomic.LoadUint64(&stats.TotalBytesSent),
				},
				Rules:   make(map[string]*DimensionStats),
				Users:   make(map[string]*DimensionStats),
				Servers: make(map[string]*DimensionStats),
				Tunnels: make(map[string]*DimensionStats),
				Proxies: make(map[string]*DimensionStats),
			}

			// Calculate global QPS
			uptime := time.Since(stats.StartTime).Seconds()
			if uptime > 0 {
				snapshot.Global.RequestsPerSecond = float64(snapshot.Global.TotalRequests) / uptime
			}

			// Collect dimensional stats - Rules
			stats.RuleStats.Range(func(key, value interface{}) bool {
				k := key.(string)
				m := value.(*Metrics)
				snapshot.Rules[k] = extractDimensionStats(m)
				return true
			})

			// Collect dimensional stats - Users
			stats.UserStats.Range(func(key, value interface{}) bool {
				k := key.(string)
				m := value.(*Metrics)
				snapshot.Users[k] = extractDimensionStats(m)
				return true
			})

			// Collect dimensional stats - Servers
			stats.ServerStats.Range(func(key, value interface{}) bool {
				k := key.(string)
				m := value.(*Metrics)
				snapshot.Servers[k] = extractDimensionStats(m)
				return true
			})

			// Collect dimensional stats - Tunnels
			stats.TunnelStats.Range(func(key, value interface{}) bool {
				k := key.(string)
				m := value.(*Metrics)
				snapshot.Tunnels[k] = extractDimensionStats(m)
				return true
			})

			// Collect dimensional stats - Proxies
			stats.ProxyStats.Range(func(key, value interface{}) bool {
				k := key.(string)
				m := value.(*Metrics)
				snapshot.Proxies[k] = extractDimensionStats(m)
				return true
			})

			// Collect dimensional stats - IPs (Top 200)
			snapshot.IPs = stats.GetTopIPsAsDimensionStats(200)

			// Save to DB
			if err := statsDB.AddSnapshot(snapshot); err != nil {
				log.Printf("[STATS] Error saving snapshot to DB: %v", err)
			}
		}
	}()
}

// extractDimensionStats extracts dimension-specific statistics from Metrics
func extractDimensionStats(m *Metrics) *DimensionStats {
	requestCount := atomic.LoadUint64(&m.RequestCount)
	totalBytesRecv := atomic.LoadUint64(&m.BytesRecv.Total)
	totalBytesSent := atomic.LoadUint64(&m.BytesSent.Total)
	totalResponseTime := atomic.LoadInt64(&m.ResponseTime.Total)

	var avgRespTime float64
	if requestCount > 0 {
		avgRespTime = float64(totalResponseTime) / float64(requestCount) / 1e6 // Convert to ms
	}

	return &DimensionStats{
		Requests:    requestCount,
		BytesRecv:   totalBytesRecv,
		BytesSent:   totalBytesSent,
		Errors:      atomic.LoadUint64(&m.Errors),
		AvgRespTime: avgRespTime,

		// Protocol-specific statistics
		HTTPRequests:    atomic.LoadUint64(&m.HTTPRequests),
		HTTPSuccess:     atomic.LoadUint64(&m.HTTPSuccess),
		HTTPFailure:     atomic.LoadUint64(&m.HTTPFailure),
		RawTCPRequests:  atomic.LoadUint64(&m.RawTCPRequests),
		HTTPBytesSent:   atomic.LoadUint64(&m.HTTPBytesSent),
		HTTPBytesRecv:   atomic.LoadUint64(&m.HTTPBytesRecv),
		RawTCPBytesSent: atomic.LoadUint64(&m.RawTCPBytesSent),
		RawTCPBytesRecv: atomic.LoadUint64(&m.RawTCPBytesRecv),
	}
}

// startLogCleanup starts a goroutine that periodically cleans up old logs
func startLogCleanup(config *Config, loggingDB *LoggingDB) {
	ticker := time.NewTicker(1 * time.Hour)
	go func() {
		for range ticker.C {
			// Get maximum retention hours from all dimensions
			// This ensures we don't delete logs that should still be retained
			retentionHours := getMaxRetentionHours(config)

			if err := loggingDB.CleanupOldLogs(retentionHours); err != nil {
				log.Printf("[LOGGING] Error cleaning up old logs: %v", err)
			} else {
				DebugLog("[LOGGING] Cleanup completed, retention=%d hours", retentionHours)
			}
		}
	}()
}
